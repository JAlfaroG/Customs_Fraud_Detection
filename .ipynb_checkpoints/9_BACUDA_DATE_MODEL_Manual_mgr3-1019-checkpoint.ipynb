{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual of DATE (Duat-Attentive-Tree-aware-Embedding) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Updated on: 2020. 6. 3.\n",
    "* Written by Yeon Soo Choi, Technical Officer, Research Unit, WCO\n",
    "* Original research paper\n",
    "    * Title: DATE: Dual Attentive Tree-aware Embedding for Customs Frauds Detection\n",
    "    * Authors\n",
    "        - IBS: Sundong Kim, Karandeep Singh, Meeyoung Cha\n",
    "        - NCKU: Yu-Che Tsai, Cheng-Te Li\n",
    "        - WCO: Yeon Soo Choi\n",
    "        - NCS: Etim Ibok\n",
    "    * Source: https://github.com/Roytsai27/Dual-Attentive-Tree-aware-Embedding\n",
    "        - All the python codes are included in this one notebook.\n",
    "* Data: T import data (xxx at transaction-item level)\n",
    "\n",
    "\n",
    "## Summary (in non-technical terms)\n",
    "\n",
    "**DATE (Dual-Attentive-Tree-aware-Embedding)** is the neural networks model to detect undervalued imports.  \n",
    "\n",
    "Imagine that you **(\"neural networks\")** are the head of a Customs targeting centre composed of **100 risk analysts (\"decision trees\")**. For a given import, you task the analysts with reporting **the probability of undervaluation** and **the estimate of additional revenue from the inspection (\"dual-task\")**.  \n",
    "\n",
    "How would you put 100 different reports together in making your final decision?\n",
    "Simply averaging their predictions may neglect some valuable information hidden in 100 reports. The DATE model help you keep all the information while paying more **ATTENTION** to specific pieces. Firstly, if there are a majority group of reports significantly similar to each other, you may pay more **ATTENTION** to those reports. Secondly, if you have analysts specialized in the specific HS code and importer of the given import, you may pay more **ATTENTION** to their reports. In the end, your final decision reflects the reports attracting your higher attention. \n",
    "\n",
    "WCO press release is available at:  \n",
    "http://www.wcoomd.org/en/media/newsroom/2020/may/wco-bacuda-experts-develop-and-share-a-neural-network-model.aspx\n",
    "\n",
    "## Summary (in technical terms)\n",
    "\n",
    "Now, lets take an overview of the model with some technical terms. For a given import, the DATE model works in the following steps;\n",
    "\n",
    "* **XGBoost**: The model passes the import into a XGBoost model which constructs multiple(eg. 100) decision trees. \n",
    "* **Embedding**: Each tree's decision (decision path, leaf-id) is transformed into a set of numbers to be fed into neural networks. \n",
    "* **Multi-head Self-attention**\n",
    "    - Self-attention: The numeric value (importance, weight) of each leaf-id is adjusted based on its correlation/interaction with other leaf-ids.\n",
    "    - Multi-head: Self-iteration is repeated in multiple times to achieve its robustness.\n",
    "* **Attention**: The numeric values (importance, weight) of each leaf-id is re-adjusted based on its correlation/interaction with the given importer-id and item-id(HScode).\n",
    "\n",
    "## OUTLINE\n",
    "* [Part 1. Preprocess data](#part1)\n",
    "* [Part 2. XGBoost model](#part2)\n",
    "* [Part 3. DATE (XGBoost + Neural Networks + Attention)](#part3)\n",
    "* [Part 4. Evaluation](#part4)\n",
    "* [Part 5. Comparison with XGBoost + Logistic Regression model](#part5)\n",
    "* [Part 6. Practice of functions](#part6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Preprocess <a id='part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "pd.set_option('display.max_columns',100)\n",
    "from collections import defaultdict\n",
    "from itertools import islice, combinations\n",
    "from datetime import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Load and clean data\n",
    "### 1.2.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_raw = pd.read_csv('./synthetic_data2.csv')\n",
    "df_raw = pd.read_csv('../data/mgr3-1019.csv', low_memory=False)\n",
    "\n",
    "# only imports\n",
    "df_raw = df_raw[df_raw['destinacion_mercancia'] == 'IM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['identificador', 'anno', 'anno_mes_registro', 'aduana', 'numero_serial',\n",
       "       'numero_registro', 'numero_declaracion', 'ingreso_salida',\n",
       "       'destinacion_mercancia', 'patron_operacion', 'regimen',\n",
       "       'agente_aduanas', 'digitador', 'agencia_carga', 'puerto_embarque',\n",
       "       'pais_procedencia', 'contribuyente', 'almacen', 'deposito_fiscal',\n",
       "       'tipo_transporte', 'tipo_pago', 'codigo_banco', 'version_declaracion',\n",
       "       'oficial_aduanero', 'canal_selectividad', 'item', 'inciso_arancelario',\n",
       "       'inciso_arancelario_old', 'pais_origen_destino',\n",
       "       'pais_origen_destino_old', 'cuantia', 'unidad_medida',\n",
       "       'preferencia_arancelaria', 'identificacion_carga', 'proveedor',\n",
       "       'consolidadora', 'fecha_arribo', 'fecha_ingreso_almacen',\n",
       "       'fecha_registro_dua', 'fecha_pago', 'fecha_recepcion_fisica',\n",
       "       'fecha_asignacion_canal', 'fecha_autorizacion_salida', 'fecha_salida',\n",
       "       'cif', 'fob', 'flete', 'seguro', 'otros_valores', 'peso_bruto',\n",
       "       'peso_neto', 'dai_nominal', 'iva_nominal', 'alc_nominal',\n",
       "       'otros_tributo_nominal', 'dai_pagado', 'iva_pagado', 'alc_pagado',\n",
       "       'otros_tributo_pagado', 'cif_variacion', 'dai_variacion',\n",
       "       'iva_variacion', 'alc_variacion', 'otros_tributo_variacion',\n",
       "       'peso_bruto_variacion', 'cantidad_unidad_variacion', 'dm_precendente',\n",
       "       'indicador_valores', 'indicador_tributos', 'indicador_clasificacion',\n",
       "       'indicador_origen', 'indicador_cantidad', 'indicador_peso',\n",
       "       'indicador_incidencia', 'modalidad_pago', 'sub_regimen',\n",
       "       'variacion_administrativa', 'variacion_tributaria',\n",
       "       'estado_declaracion', 'actividad_economica', 'manifiesto_carga',\n",
       "       'codigo_proveedor', 'impuesto_total', 'illicit', 'revenue', 'mes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = [\n",
    "    'anno', 'mes', 'aduana', \n",
    "    #'agente_aduanas',\n",
    "    'contribuyente', 'inciso_arancelario', 'pais_origen_destino', \n",
    "    'cif', 'cuantia', 'peso_bruto','impuesto_total', 'revenue','illicit'\n",
    "]\n",
    "\n",
    "df=df[columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anno</th>\n",
       "      <th>mes</th>\n",
       "      <th>aduana</th>\n",
       "      <th>contribuyente</th>\n",
       "      <th>inciso_arancelario</th>\n",
       "      <th>pais_origen_destino</th>\n",
       "      <th>cif</th>\n",
       "      <th>cuantia</th>\n",
       "      <th>peso_bruto</th>\n",
       "      <th>impuesto_total</th>\n",
       "      <th>revenue</th>\n",
       "      <th>illicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2043244</th>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>83</td>\n",
       "      <td>12171306680010</td>\n",
       "      <td>87032190000</td>\n",
       "      <td>IN</td>\n",
       "      <td>7199.07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>870.0</td>\n",
       "      <td>2562.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043245</th>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>83</td>\n",
       "      <td>06140104740014</td>\n",
       "      <td>87043161000</td>\n",
       "      <td>ID</td>\n",
       "      <td>48039.19</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5875.0</td>\n",
       "      <td>8959.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043246</th>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>83</td>\n",
       "      <td>12171306680010</td>\n",
       "      <td>87042151000</td>\n",
       "      <td>MX</td>\n",
       "      <td>158725.54</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16530.0</td>\n",
       "      <td>29602.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043247</th>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>83</td>\n",
       "      <td>12171306680010</td>\n",
       "      <td>87032369000</td>\n",
       "      <td>MX</td>\n",
       "      <td>115010.39</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8968.0</td>\n",
       "      <td>47441.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043248</th>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>83</td>\n",
       "      <td>12171306680010</td>\n",
       "      <td>87042290000</td>\n",
       "      <td>JP</td>\n",
       "      <td>159813.84</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20220.0</td>\n",
       "      <td>22581.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         anno  mes  aduana   contribuyente  inciso_arancelario  \\\n",
       "2043244  2019   10      83  12171306680010         87032190000   \n",
       "2043245  2019   10      83  06140104740014         87043161000   \n",
       "2043246  2019   11      83  12171306680010         87042151000   \n",
       "2043247  2019   11      83  12171306680010         87032369000   \n",
       "2043248  2019   11      83  12171306680010         87042290000   \n",
       "\n",
       "        pais_origen_destino        cif  cuantia  peso_bruto  impuesto_total  \\\n",
       "2043244                  IN    7199.07      1.0       870.0         2562.86   \n",
       "2043245                  ID   48039.19      5.0      5875.0         8959.29   \n",
       "2043246                  MX  158725.54     10.0     16530.0        29602.30   \n",
       "2043247                  MX  115010.39      8.0      8968.0        47441.77   \n",
       "2043248                  JP  159813.84      4.0     20220.0        22581.68   \n",
       "\n",
       "         revenue  illicit  \n",
       "2043244      0.0        0  \n",
       "2043245      0.0        0  \n",
       "2043246      0.0        0  \n",
       "2043247      0.0        0  \n",
       "2043248      0.0        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Finish loading data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Define functions to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. \"merge_attributes\" function [(link to practice)](#practice1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"merge_attributes\" fuction is to create a new categorical variable by combining multiple existing categorical variables into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_attributes(df: pd.DataFrame, *args: str) -> None: \n",
    "    # Note: \"*args\" represents multiple arguments, i.e. multiple variable names could come. \n",
    "    # Note: \"-> None\" represents that this function returns None (i.e. type annotation)\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    dtype *args: strings (attribute names that want to be combined)\n",
    "    \"\"\"\n",
    "    # To set data type of each argument as string\n",
    "    iterables = [df[arg].astype(str) for arg in args] \n",
    "    # To name the newly combined variable/column\n",
    "    columnName = '&'.join([*args]) \n",
    "    # To create a column for the combined variable\n",
    "    fs = [''.join([v for v in var]) for var in zip(*iterables)] # \"*\" represents \"unzip\"\n",
    "    df.loc[:, columnName] = fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. \"preprocess\" function [(link to practice)](#practice2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fuction is to;\n",
    "* generate additional features such as unitprice, weight-unitprice and effective tariff rate;\n",
    "* merge some attributes, using the above \"merge_attributes\" function; and \n",
    "* generate date-related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7          4011\n",
       "8          4011\n",
       "9          4011\n",
       "10         4011\n",
       "11         4011\n",
       "           ... \n",
       "2043244    8703\n",
       "2043245    8704\n",
       "2043246    8704\n",
       "2043247    8703\n",
       "2043248    8704\n",
       "Name: inciso_arancelario, Length: 1585557, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['inciso_arancelario'].apply(lambda x: int(x) // 100000).astype(str).apply(lambda x: int(x) // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Note: \"-> pd.DataFrame\" represents that this function returns a dataframe.\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    rtype df: dataframe\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=['cif', 'impuesto_total'])\n",
    "    df.loc[:, 'Unitprice'] = df['cif']/df['cuantia']\n",
    "    df.loc[:, 'WUnitprice'] = df['cif']/df['peso_bruto']\n",
    "    df.loc[:, 'TaxRatio'] = df['impuesto_total'] / df['cif']\n",
    "    df.loc[:, 'TaxUnitquantity'] = df['impuesto_total'] / df['cuantia']\n",
    "    df.loc[:, 'HS6'] = df['inciso_arancelario'].apply(lambda x: int(x) // 100000).astype(str) #HS10digit\n",
    "    df.loc[:, 'HS4'] = df['HS6'].apply(lambda x: int(x) // 100)\n",
    "    df.loc[:, 'HS2'] = df['HS4'].apply(lambda x: int(x) // 100)\n",
    "    \n",
    "    # Made a general function \"merge_attributes\" for supporting any combination    \n",
    "    merge_attributes(df, 'HS6','pais_origen_destino')\n",
    "    merge_attributes(df, 'aduana','contribuyente')\n",
    "    merge_attributes(df, 'aduana','HS6')\n",
    "    merge_attributes(df, 'aduana','pais_origen_destino')\n",
    "    # another way of combining features\n",
    "    #df.loc[:, 'HS6.Origin'] = [str(i)+'&'+j for i, j in zip(df['HS6'], df['ORIGIN'])]\n",
    "    \n",
    "    \n",
    "    '''# Day of Year of SGD.DATE\n",
    "    tmp2 = {}\n",
    "    for date in set(df['SGD.DATE']):\n",
    "        tmp2[date] = dt.strptime(date, '%Y%m%d') # Capital letter Y\n",
    "    tmp_day = {}\n",
    "    tmp_week = {}\n",
    "    tmp_month = {}\n",
    "    yearStart = dt(tmp2[date].date().year, 1, 1)\n",
    "    for item in tmp2:\n",
    "        tmp_day[item] = (tmp2[item] - yearStart).days\n",
    "        tmp_week[item] = int(tmp_day[item] / 7)\n",
    "        tmp_month[item] = int(tmp_day[item] / 30)\n",
    "        \n",
    "    df.loc[:, 'SGD.DayofYear'] = df['SGD.DATE'].apply(lambda x: tmp_day[x])\n",
    "    df.loc[:, 'SGD.WeekofYear'] = df['SGD.DATE'].apply(lambda x: tmp_week[x])\n",
    "    df.loc[:, 'SGD.MonthofYear'] = df['SGD.DATE'].apply(lambda x: tmp_month[x])'''\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. \"find_risk_profile\" function [(link to practice)](#practice3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to identify/calculate risk-profiling indicators of the features;\n",
    "* option 1 (topk): Lists of top n high-risk categories in the features\n",
    "* option 2 (ratio): illicit ratio of categories in the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_risk_profile(df: pd.DataFrame, \n",
    "                      feature: str, \n",
    "                      topk_ratio: float, \n",
    "                      adj: float, \n",
    "                      option: str) -> list or dict:\n",
    "    \"\"\"\n",
    "    dtype feature: str\n",
    "    dtype topk_ratio: float (range: 0-1)\n",
    "    dtype adj: float (to modify the mean)\n",
    "    dtype option: str ('topk', 'ratio')\n",
    "    rtype: list(option='topk') or dict(option='ratio')\n",
    "    \n",
    "    The option topk is usually better than the ratio because of overfitting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Top-k suspicious item flagging\n",
    "    if option == 'topk':\n",
    "        # Group data by a specified feature(vairable)\n",
    "        total_cnt = df.groupby([feature])['illicit']\n",
    "        # Set the number of entities to be included a black list.\n",
    "        nrisky_profile = int(topk_ratio*len(total_cnt))+1\n",
    "        # For each entity, calculate 'total number of frauds' divided by 'total number of imports' \n",
    "        adj_prob_illicit = total_cnt.sum() / (total_cnt.count()+adj)  # Smoothed mean\n",
    "        return list(adj_prob_illicit.sort_values(ascending=False).head(nrisky_profile).index)\n",
    "    \n",
    "    # Illicit-ratio encoding (Mean target encoding)\n",
    "    # Refer: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/target-encoding.html\n",
    "    # Refer: https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0\n",
    "    elif option == 'ratio':\n",
    "        # For target encoding, we just use 70% of train data to avoid overfitting (otherwise, test AUC drops significantly)\n",
    "        total_cnt = df.sample(frac=0.7).groupby([feature])['illicit']\n",
    "        # prob_illicit = total_cnt.mean()  # Simple mean\n",
    "        adj_prob_illicit = total_cnt.sum() / (total_cnt.count()+adj)  # Smoothed mean\n",
    "        return adj_prob_illicit.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. \"tag_risky_profiles\" function [(link to practice)](#practice4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to generate risk-profiling tags of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_risky_profiles(df: pd.DataFrame, \n",
    "                       profile: str, \n",
    "                       profiles: list or dict, \n",
    "                       option: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    dtype profile: str\n",
    "    dtype profiles: list(option='topk') or dictionary(option='ratio')\n",
    "    dtype option: str ('topk', 'ratio')\n",
    "    rtype: dataframe\n",
    "    \n",
    "    The option topk is usually better than the ratio because of overfitting.\n",
    "    \"\"\"\n",
    "    # Top-k suspicious item flagging\n",
    "    if option == 'topk':\n",
    "        d = defaultdict(int) # return 0 for not-defined keys\n",
    "        for id in profiles:\n",
    "            d[id] = 1\n",
    "    #     print(list(islice(d.items(), 10)))  # For debugging\n",
    "        df.loc[:, 'RiskH.'+profile] = df[profile].apply(lambda x: d[x])\n",
    "    \n",
    "    # Illicit-ratio encoding\n",
    "    elif option == 'ratio':\n",
    "        overall_ratio_train = np.mean(train.illicit) # When scripting, saving it as a class variable is clearer.\n",
    "        df.loc[:, 'RiskH.'+profile] = df[profile].apply(lambda x: profiles.get(x,overall_ratio_train))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.4. Preprocess data with pre-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset settings\n",
    "data_length = df.shape[0]\n",
    "train_ratio = 0.6\n",
    "valid_ratio = 0.8\n",
    "train_length = int(data_length*train_ratio)\n",
    "valid_length = int(data_length*valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/valid/test set\n",
    "#df = df.sort_values('SGD.DATE')\n",
    "train = df.iloc[:train_length,:]\n",
    "valid = df.iloc[train_length:valid_length,:]\n",
    "test = df.iloc[valid_length:,:]\n",
    "#train = df[df[\"SGD.DATE\"] < \"2015-12-01\"]\n",
    "#valid = df[(df[\"SGD.DATE\"] >= \"2015-12-01\") & (df[\"SGD.DATE\"] < \"2016-01-01\")]\n",
    "#test = df[df[\"SGD.DATE\"] >= \"2016-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((951334, 12), (317111, 12), (317112, 12))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anno', 'mes', 'aduana', 'contribuyente', 'inciso_arancelario',\n",
       "       'pais_origen_destino', 'cif', 'cuantia', 'peso_bruto', 'impuesto_total',\n",
       "       'revenue', 'illicit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save label data\n",
    "train_reg_label = train['revenue'].values\n",
    "valid_reg_label = valid['revenue'].values\n",
    "test_reg_label = test['revenue'].values\n",
    "train_cls_label = train[\"illicit\"].values\n",
    "valid_cls_label = valid[\"illicit\"].values\n",
    "test_cls_label = test[\"illicit\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing\n",
    "train = preprocess(train)\n",
    "valid = preprocess(valid)\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anno', 'mes', 'aduana', 'contribuyente', 'inciso_arancelario',\n",
       "       'pais_origen_destino', 'cif', 'cuantia', 'peso_bruto', 'impuesto_total',\n",
       "       'revenue', 'illicit', 'Unitprice', 'WUnitprice', 'TaxRatio',\n",
       "       'TaxUnitquantity', 'HS6', 'HS4', 'HS2', 'HS6&pais_origen_destino',\n",
       "       'aduana&contribuyente', 'aduana&HS6', 'aduana&pais_origen_destino'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a few more risky profiles\n",
    "risk_profiles = {}\n",
    "profile_candidates = ['contribuyente',\n",
    "                      'pais_origen_destino', #'LAST_DEPARTURE.CODE', 'CONTRACT_PARTY.CODE',\n",
    "                      'inciso_arancelario', 'cuantia', 'HS6', 'HS4', 'HS2', 'aduana'] + [col for col in train.columns if '&' in col]\n",
    "\n",
    "for profile in profile_candidates:\n",
    "    option = 'topk'\n",
    "    risk_profiles[profile] = find_risk_profile(train, profile, 0.1, 10, option=option)\n",
    "    train = tag_risky_profiles(train, profile, risk_profiles[profile], option=option)\n",
    "    valid = tag_risky_profiles(valid, profile, risk_profiles[profile], option=option)\n",
    "    test = tag_risky_profiles(test, profile, risk_profiles[profile], option=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anno', 'mes', 'aduana', 'contribuyente', 'inciso_arancelario',\n",
       "       'pais_origen_destino', 'cif', 'cuantia', 'peso_bruto', 'impuesto_total',\n",
       "       'revenue', 'illicit', 'Unitprice', 'WUnitprice', 'TaxRatio',\n",
       "       'TaxUnitquantity', 'HS6', 'HS4', 'HS2', 'HS6&pais_origen_destino',\n",
       "       'aduana&contribuyente', 'aduana&HS6', 'aduana&pais_origen_destino',\n",
       "       'RiskH.contribuyente', 'RiskH.pais_origen_destino',\n",
       "       'RiskH.inciso_arancelario', 'RiskH.cuantia', 'RiskH.HS6', 'RiskH.HS4',\n",
       "       'RiskH.HS2', 'RiskH.aduana', 'RiskH.HS6&pais_origen_destino',\n",
       "       'RiskH.aduana&contribuyente', 'RiskH.aduana&HS6',\n",
       "       'RiskH.aduana&pais_origen_destino'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to use in a classifier\n",
    "column_to_use = ['cif', 'impuesto_total', 'peso_bruto', 'cuantia',  \n",
    "                 'Unitprice', 'WUnitprice', 'TaxRatio', 'TaxUnitquantity', 'inciso_arancelario', \n",
    "                 'HS6', 'HS4', 'HS2', 'anno', 'mes'] + [col for col in train.columns if 'RiskH' in col] \n",
    "\n",
    "# Extract only numeric values from data to be fed into models\n",
    "X_train = train[column_to_use].values\n",
    "X_valid = valid[column_to_use].values\n",
    "X_test = test[column_to_use].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute nan\n",
    "X_train = np.nan_to_num(X_train, 0)\n",
    "X_valid = np.nan_to_num(X_valid, 0)\n",
    "X_test = np.nan_to_num(X_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data size...\n",
      "(951334, 26) (951334,) (951334,)\n",
      "(317111, 26) (317111,) (317111,)\n",
      "(317112, 26) (317112,) (317112,)\n"
     ]
    }
   ],
   "source": [
    "# make sure the data size are correct\n",
    "print(\"Checking data size...\")\n",
    "print(X_train.shape, train_cls_label.shape, train_reg_label.shape)\n",
    "print(X_valid.shape, valid_cls_label.shape, valid_reg_label.shape)\n",
    "print(X_test.shape, test_cls_label.shape, test_reg_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Save all the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all data in a dictionary\n",
    "all_data = {\"raw\":{\"train\":train,\"valid\":valid,\"test\":test},\n",
    " \"xgboost_data\":{\"train_x\":X_train,\"train_y\":train_cls_label,\\\n",
    "                 \"valid_x\":X_valid,\"valid_y\":valid_cls_label,\\\n",
    "                 \"test_x\":X_test,\"test_y\":test_cls_label},\n",
    " \"revenue\":{\"train\":train_reg_label,\"valid\":valid_reg_label,\"test\":test_reg_label}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking label distribution\n",
      "Training: 0.04286523917735749\n",
      "Validation: 0.04835281088318429\n",
      "Testing: 0.04747307920988307\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Checking label distribution\")\n",
    "cnt = Counter(train_cls_label)\n",
    "print(\"Training:\",cnt[1]/cnt[0])\n",
    "cnt = Counter(valid_cls_label)\n",
    "print(\"Validation:\",cnt[1]/cnt[0])\n",
    "cnt = Counter(test_cls_label)\n",
    "print(\"Testing:\",cnt[1]/cnt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle a variable to a file\n",
    "# reference for pickle: https://www.datacamp.com/community/tutorials/pickle-python-tutorial\n",
    "file = open('./processed_data.pickle', 'wb')\n",
    "pickle.dump(all_data, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. XGBoost model <a id='part2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install xgboost -> https://stackoverflow.com/questions/55332943/importerror-no-module-named-xgboost\n",
    "# Install pytorch -> https://pytorch.org/get-started/locally/#mac-anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle\n",
    "import copy\n",
    "import os \n",
    "from xgboost import XGBClassifier\n",
    "#(toberemoved)from utils import find_best_threshold,process_leaf_idx,stratify_sample\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Load the preprocessed data in Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['raw', 'xgboost_data', 'revenue'])\n",
      "Finish loading data...\n"
     ]
    }
   ],
   "source": [
    "# load preprocessed data\n",
    "# with open(\"./processed_data_13-01-01.pickle\",\"rb\") as f :\n",
    "#     processed_data = pickle.load(f)\n",
    "with open(\"./processed_data.pickle\",\"rb\") as f :\n",
    "    processed_data = pickle.load(f)\n",
    "print(processed_data.keys())\n",
    "print(\"Finish loading data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test data \n",
    "train = processed_data[\"raw\"][\"train\"]\n",
    "valid = processed_data[\"raw\"][\"valid\"]\n",
    "test = processed_data[\"raw\"][\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split labels into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue data for regression target \n",
    "revenue_train = processed_data[\"revenue\"][\"train\"]\n",
    "revenue_valid = processed_data[\"revenue\"][\"valid\"]\n",
    "revenue_test = processed_data[\"revenue\"][\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take logged values of revenue, and normalize them. --> getting more densed distribution/minimizing outliers' impacts.  \n",
    "As we assume no information on valid-data and test-data, they are normalized with the information of train-data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize revenue by f(x) = log(x+1)/max(xi)\n",
    "norm_revenue_train = np.log(revenue_train+1)\n",
    "norm_revenue_valid = np.log(revenue_valid+1)\n",
    "norm_revenue_test = np.log(revenue_test+1) \n",
    "global_max = max(norm_revenue_train) \n",
    "norm_revenue_train = norm_revenue_train/global_max\n",
    "norm_revenue_valid = norm_revenue_valid/global_max\n",
    "norm_revenue_test = norm_revenue_test/global_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split xgboost data into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost data \n",
    "xgb_trainx = processed_data[\"xgboost_data\"][\"train_x\"]\n",
    "xgb_trainy = processed_data[\"xgboost_data\"][\"train_y\"]\n",
    "xgb_validx = processed_data[\"xgboost_data\"][\"valid_x\"]\n",
    "xgb_validy = processed_data[\"xgboost_data\"][\"valid_y\"]\n",
    "xgb_testx = processed_data[\"xgboost_data\"][\"test_x\"]\n",
    "xgb_testy = processed_data[\"xgboost_data\"][\"test_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Define functions to be used\n",
    "### 2.3.1. \"find_best_threshod\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(model,x_list,y_test,best_thresh = None):\n",
    "    '''\n",
    "    This function is to find the best threshold to determine \"to inspect\" or \"not\".\n",
    "    We assume that we inspect only the imports where predicted probability of fraud is above the threshold.\n",
    "    The input arguments are;\n",
    "    - dtype model: scikit-learn classifier model\n",
    "    - dtype x_list: list or array of features (data)\n",
    "    - dtype y_test: array of true labels \n",
    "    '''\n",
    "    # Predict probability of fraud of each import.\n",
    "    y_pred_prob = model.predict_proba(x_list)[:,1]\n",
    "    # Set threshold range as [0.1, 0.2, ..., 0.5]. \n",
    "    threshold_list = np.arange(0.1,0.6,0.1)\n",
    "    # Set an initial value of best threshold.\n",
    "    best_f1 = 0\n",
    "    # if best_thresh is set as \"None\", this function is to find the best_thresh as well as best_f1 \n",
    "    if best_thresh ==None:\n",
    "        for th in threshold_list:\n",
    "            y_pred_label = (y_pred_prob > th)*1 \n",
    "            f_score = f1_score(y_test,y_pred_label)\n",
    "            if f_score > best_f1:\n",
    "                best_f1 = f_score\n",
    "                best_thresh = th \n",
    "        return best_thresh, best_f1\n",
    "    # if best_thresh is set as a certain number, this function is to calculate its f1 score.\n",
    "    else:\n",
    "        y_pred_label = (y_pred_prob > best_thresh)*1 \n",
    "        best_f1 = f1_score(y_test,y_pred_label)\n",
    "    print(\"F1-score equals to:%.4f\"%(best_f1))\n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Deploy a XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training xgboost model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, eval_metric='logloss',\n",
       "              gamma=0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=4, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=-1,\n",
       "              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "              scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training xgboost model...\")\n",
    "# Convert nparray to pd.DataFrame (why? not necessary)\n",
    "\n",
    "column_to_use = ['cif', 'impuesto_total', 'peso_bruto', 'cuantia',  \n",
    "                 'Unitprice', 'WUnitprice', 'TaxRatio', 'TaxUnitquantity', 'inciso_arancelario', \n",
    "                 'HS6', 'HS4', 'HS2', 'anno', 'mes'] + [col for col in train.columns if 'RiskH' in col] \n",
    "\n",
    "columns = column_to_use\n",
    "xgb_trainx = pd.DataFrame(xgb_trainx,columns=columns).values\n",
    "xgb_validx = pd.DataFrame(xgb_validx,columns=columns).values\n",
    "xgb_testx = pd.DataFrame(xgb_testx,columns=columns).values\n",
    "# Initiate the model\n",
    "xgb_clf = XGBClassifier(n_estimators=100, max_depth=4, n_jobs=-1, eval_metric='logloss')\n",
    "# Train/fit the model\n",
    "xgb_clf.fit(xgb_trainx, xgb_trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first tree out of 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install graphviz -> https://anaconda.org/conda-forge/python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"2745pt\" height=\"392pt\"\r\n",
       " viewBox=\"0.00 0.00 2745.39 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-388 2741.39,-388 2741.39,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1437.99\" cy=\"-366\" rx=\"38.9931\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1437.99\" y=\"-362.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f23&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1025.99\" cy=\"-279\" rx=\"38.9931\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1025.99\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f24&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1403.22,-357.826C1328.64,-342.438 1153.36,-306.277 1070.67,-289.217\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1071.32,-285.777 1060.82,-287.185 1069.9,-292.633 1071.32,-285.777\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1291.49\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1703.99\" cy=\"-279\" rx=\"38.9931\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1703.99\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f14&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1469.03,-355.083C1517.05,-339.737 1609.4,-310.227 1662.91,-293.129\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1664.29,-296.363 1672.75,-289.985 1662.15,-289.695 1664.29,-296.363\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1594.49\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"502.993\" cy=\"-192\" rx=\"38.9931\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"502.993\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f22&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M989.632,-272.09C896.913,-257.021 651.761,-217.178 549.167,-200.504\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"549.545,-197.02 539.113,-198.87 548.422,-203.929 549.545,-197.02\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"830.493\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1025.99\" cy=\"-192\" rx=\"69.5877\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1025.99\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f6&lt;0.300100863</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>1&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1025.99,-260.799C1025.99,-249.163 1025.99,-233.548 1025.99,-220.237\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1029.49,-220.175 1025.99,-210.175 1022.49,-220.175 1029.49,-220.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1033.49\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node18\" class=\"node\"><title>5</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1703.99\" cy=\"-192\" rx=\"38.9931\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1703.99\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f17&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;5 -->\r\n",
       "<g id=\"edge17\" class=\"edge\"><title>2&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1703.99,-260.799C1703.99,-249.163 1703.99,-233.548 1703.99,-220.237\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1707.49,-220.175 1703.99,-210.175 1700.49,-220.175 1707.49,-220.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1738.49\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node19\" class=\"node\"><title>6</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2210.99\" cy=\"-192\" rx=\"38.9931\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2210.99\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f17&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;6 -->\r\n",
       "<g id=\"edge18\" class=\"edge\"><title>2&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1740.05,-271.955C1830.07,-256.863 2064.62,-217.54 2164.68,-200.765\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2165.5,-204.176 2174.78,-199.071 2164.34,-197.273 2165.5,-204.176\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1995.49\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>7</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"246.993\" cy=\"-105\" rx=\"69.5877\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"246.993\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f6&lt;0.422516406</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;7 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M472.368,-180.831C429.228,-166.508 350.026,-140.21 297.939,-122.916\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"298.816,-119.519 288.223,-119.69 296.61,-126.162 298.816,-119.519\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"425.493\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>8</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"502.993\" cy=\"-105\" rx=\"69.5877\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"502.993\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f6&lt;0.300065219</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;8 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>3&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M502.993,-173.799C502.993,-162.163 502.993,-146.548 502.993,-133.237\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"506.493,-133.175 502.993,-123.175 499.493,-133.175 506.493,-133.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"510.493\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>9</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"937.993\" cy=\"-105\" rx=\"69.5877\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"937.993\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f6&lt;0.130160347</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;9 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>4&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1008.61,-174.207C995.384,-161.435 977.025,-143.701 962.341,-129.518\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"964.668,-126.899 955.043,-122.469 959.805,-131.934 964.668,-126.899\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1022.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>10</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1111.99\" cy=\"-105\" rx=\"74.187\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1111.99\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f4&lt;0.0145366797</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;10 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>4&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1042.98,-174.207C1055.82,-161.517 1073.62,-143.93 1087.92,-129.793\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1090.39,-132.277 1095.04,-122.758 1085.46,-127.299 1090.39,-132.277\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1082.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 15 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>15</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"77.9931\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"77.9931\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.581871271</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;15 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>7&#45;&gt;15</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M214.901,-88.8948C202.125,-82.8082 187.319,-75.6624 173.993,-69 155.131,-59.5697 134.359,-48.7992 116.975,-39.6757\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"118.18,-36.3545 107.7,-34.7955 114.92,-42.5493 118.18,-36.3545\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"208.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 16 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>16</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"246.993\" cy=\"-18\" rx=\"73.387\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"246.993\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.54190892</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;16 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>7&#45;&gt;16</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M246.993,-86.799C246.993,-75.1626 246.993,-59.5479 246.993,-46.2368\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"250.493,-46.1754 246.993,-36.1754 243.493,-46.1755 250.493,-46.1754\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"254.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 17 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>17</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"415.993\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"415.993\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.546742141</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;17 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>8&#45;&gt;17</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M485.805,-87.2067C472.816,-74.517 454.815,-56.9297 440.345,-42.7925\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"442.745,-40.2433 433.146,-35.7584 437.853,-45.2503 442.745,-40.2433\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"499.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 18 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>18</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"589.993\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"589.993\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.423194259</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;18 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>8&#45;&gt;18</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M520.181,-87.2067C533.17,-74.517 551.171,-56.9297 565.641,-42.7925\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"568.133,-45.2503 572.84,-35.7584 563.242,-40.2433 568.133,-45.2503\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"559.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 19 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>19</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"763.993\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"763.993\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.565239131</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;19 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>9&#45;&gt;19</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M906.331,-88.9263C893.484,-82.7822 878.513,-75.5871 864.993,-69 844.981,-59.2494 822.807,-48.2895 804.345,-39.1213\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"805.847,-35.9593 795.334,-34.6423 802.731,-42.2276 805.847,-35.9593\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"899.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 20 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>20</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"937.993\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"937.993\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.497666031</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;20 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>9&#45;&gt;20</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M937.993,-86.799C937.993,-75.1626 937.993,-59.5479 937.993,-46.2368\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"941.493,-46.1754 937.993,-36.1754 934.493,-46.1755 941.493,-46.1754\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"945.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 21 -->\r\n",
       "<g id=\"node16\" class=\"node\"><title>21</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1111.99\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1111.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.597202837</text>\r\n",
       "</g>\r\n",
       "<!-- 10&#45;&gt;21 -->\r\n",
       "<g id=\"edge15\" class=\"edge\"><title>10&#45;&gt;21</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1111.99,-86.799C1111.99,-75.1626 1111.99,-59.5479 1111.99,-46.2368\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1115.49,-46.1754 1111.99,-36.1754 1108.49,-46.1755 1115.49,-46.1754\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1146.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 22 -->\r\n",
       "<g id=\"node17\" class=\"node\"><title>22</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1282.99\" cy=\"-18\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1282.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.153690308</text>\r\n",
       "</g>\r\n",
       "<!-- 10&#45;&gt;22 -->\r\n",
       "<g id=\"edge16\" class=\"edge\"><title>10&#45;&gt;22</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1144.42,-88.6643C1157.11,-82.608 1171.77,-75.5441 1184.99,-69 1204.38,-59.4073 1225.8,-48.516 1243.65,-39.3527\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1245.34,-42.42 1252.63,-34.7345 1242.14,-36.1947 1245.34,-42.42\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1222.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node20\" class=\"node\"><title>11</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1627.99\" cy=\"-105\" rx=\"65.7887\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1627.99\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f1&lt;80.2099991</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;11 -->\r\n",
       "<g id=\"edge19\" class=\"edge\"><title>5&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1688.17,-175.202C1682.36,-169.27 1675.79,-162.411 1669.99,-156 1662.7,-147.925 1654.97,-138.875 1648.15,-130.717\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1650.77,-128.392 1641.7,-122.925 1645.38,-132.858 1650.77,-128.392\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1704.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node21\" class=\"node\"><title>12</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1801.99\" cy=\"-105\" rx=\"65.7887\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1801.99\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f2&lt;17.7249985</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;12 -->\r\n",
       "<g id=\"edge20\" class=\"edge\"><title>5&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1721.51,-175.804C1736.37,-162.918 1757.92,-144.226 1774.98,-129.426\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1777.69,-131.713 1782.95,-122.516 1773.1,-126.425 1777.69,-131.713\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1766.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 13 -->\r\n",
       "<g id=\"node26\" class=\"node\"><title>13</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2210.99\" cy=\"-105\" rx=\"38.9931\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2210.99\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f24&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;13 -->\r\n",
       "<g id=\"edge25\" class=\"edge\"><title>6&#45;&gt;13</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2210.99,-173.799C2210.99,-162.163 2210.99,-146.548 2210.99,-133.237\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2214.49,-133.175 2210.99,-123.175 2207.49,-133.175 2214.49,-133.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2245.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 14 -->\r\n",
       "<g id=\"node27\" class=\"node\"><title>14</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2490.99\" cy=\"-105\" rx=\"38.9931\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2490.99\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f13&lt;4.5</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;14 -->\r\n",
       "<g id=\"edge26\" class=\"edge\"><title>6&#45;&gt;14</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2242.57,-181.415C2293.3,-166.014 2392.97,-135.757 2449.42,-118.62\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2450.55,-121.935 2459.1,-115.681 2448.52,-115.237 2450.55,-121.935\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2375.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 23 -->\r\n",
       "<g id=\"node22\" class=\"node\"><title>23</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1453.99\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1453.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.535146117</text>\r\n",
       "</g>\r\n",
       "<!-- 11&#45;&gt;23 -->\r\n",
       "<g id=\"edge21\" class=\"edge\"><title>11&#45;&gt;23</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1596.69,-89.0983C1583.76,-82.9167 1568.64,-75.6486 1554.99,-69 1534.98,-59.2494 1512.81,-48.2895 1494.35,-39.1213\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1495.85,-35.9593 1485.33,-34.6423 1492.73,-42.2276 1495.85,-35.9593\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1589.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 24 -->\r\n",
       "<g id=\"node23\" class=\"node\"><title>24</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1627.99\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1627.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.389487475</text>\r\n",
       "</g>\r\n",
       "<!-- 11&#45;&gt;24 -->\r\n",
       "<g id=\"edge22\" class=\"edge\"><title>11&#45;&gt;24</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1627.99,-86.799C1627.99,-75.1626 1627.99,-59.5479 1627.99,-46.2368\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1631.49,-46.1754 1627.99,-36.1754 1624.49,-46.1755 1631.49,-46.1754\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1635.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 25 -->\r\n",
       "<g id=\"node24\" class=\"node\"><title>25</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1801.99\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1801.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.436783582</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;25 -->\r\n",
       "<g id=\"edge23\" class=\"edge\"><title>12&#45;&gt;25</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1801.99,-86.799C1801.99,-75.1626 1801.99,-59.5479 1801.99,-46.2368\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1805.49,-46.1754 1801.99,-36.1754 1798.49,-46.1755 1805.49,-46.1754\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1836.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 26 -->\r\n",
       "<g id=\"node25\" class=\"node\"><title>26</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1976.99\" cy=\"-18\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1976.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0287425164</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;26 -->\r\n",
       "<g id=\"edge24\" class=\"edge\"><title>12&#45;&gt;26</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1833.28,-89.0716C1846.21,-82.8859 1861.33,-75.622 1874.99,-69 1895.18,-59.2112 1917.58,-48.2471 1936.22,-39.0859\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1937.9,-42.164 1945.33,-34.6113 1934.81,-35.882 1937.9,-42.164\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1911.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 27 -->\r\n",
       "<g id=\"node28\" class=\"node\"><title>27</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2147.99\" cy=\"-18\" rx=\"73.387\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2147.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.36092633</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;27 -->\r\n",
       "<g id=\"edge27\" class=\"edge\"><title>13&#45;&gt;27</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2198.85,-87.6111C2189.73,-75.3064 2177.07,-58.2339 2166.67,-44.1977\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2169.3,-41.8701 2160.53,-35.9203 2163.68,-46.0383 2169.3,-41.8701\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2218.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 28 -->\r\n",
       "<g id=\"node29\" class=\"node\"><title>28</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2316.99\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2316.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.113157898</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;28 -->\r\n",
       "<g id=\"edge28\" class=\"edge\"><title>13&#45;&gt;28</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2230.84,-89.2019C2239,-83.0362 2248.52,-75.752 2256.99,-69 2267.79,-60.4012 2279.49,-50.7356 2289.66,-42.211\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2292.05,-44.7767 2297.45,-35.6612 2287.54,-39.4198 2292.05,-44.7767\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2283.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 29 -->\r\n",
       "<g id=\"node30\" class=\"node\"><title>29</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2490.99\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2490.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.267233491</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;29 -->\r\n",
       "<g id=\"edge29\" class=\"edge\"><title>14&#45;&gt;29</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2490.99,-86.799C2490.99,-75.1626 2490.99,-59.5479 2490.99,-46.2368\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2494.49,-46.1754 2490.99,-36.1754 2487.49,-46.1755 2494.49,-46.1754\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2525.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 30 -->\r\n",
       "<g id=\"node31\" class=\"node\"><title>30</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2661.99\" cy=\"-18\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2661.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.182687014</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;30 -->\r\n",
       "<g id=\"edge30\" class=\"edge\"><title>14&#45;&gt;30</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2517.42,-91.5203C2531.32,-84.9089 2548.62,-76.6036 2563.99,-69 2583.38,-59.4073 2604.8,-48.516 2622.65,-39.3527\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2624.34,-42.42 2631.63,-34.7345 2621.14,-36.1947 2624.34,-42.42\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2601.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x244ec1b6f40>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb.to_graphviz(booster = xgb_clf, num_trees=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Evaluating xgboost model------\n",
      "F1-score equals to:0.1920\n",
      "AUC = 0.6880, F1-score = 0.1920\n"
     ]
    }
   ],
   "source": [
    "# evaluate xgboost model\n",
    "print(\"------Evaluating xgboost model------\")\n",
    "# Predict\n",
    "test_pred = xgb_clf.predict_proba(xgb_testx)[:,1]\n",
    "# Calculate auc\n",
    "xgb_auc = roc_auc_score(xgb_testy, test_pred)\n",
    "# Find the best threshold\n",
    "xgb_threshold,_ = find_best_threshold(xgb_clf, xgb_validx, xgb_validy)\n",
    "# Calculate the best f1 score\n",
    "xgb_f1 = find_best_threshold(xgb_clf, xgb_testx, xgb_testy,best_thresh=xgb_threshold)\n",
    "print(\"AUC = %.4f, F1-score = %.4f\" % (xgb_auc, xgb_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. DATE model (XGB + Neural Networks + Attention) <a id='part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Set environmenst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import time \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ranger -> https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "# You may restart your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model.AttTreeEmbedding import Attention, DATE\n",
    "from ranger import Ranger\n",
    "#from utils import torch_threshold, fgsm_attack, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Preprocessing data: Integer-encoding of IMPORTER.TIN and TARIFF.CODE for attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user & item information \n",
    "train_raw_importers = train['contribuyente'].values\n",
    "train_raw_items = train['inciso_arancelario'].values\n",
    "valid_raw_importers = valid['contribuyente'].values\n",
    "valid_raw_items = valid['inciso_arancelario'].values\n",
    "test_raw_importers = test['contribuyente']\n",
    "test_raw_items = test['inciso_arancelario']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need padding for unseen user or item \n",
    "importer_set = set(train_raw_importers)\n",
    "item_set = set(train_raw_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to +1 for zero padding \n",
    "importer_mapping = {v:i+1 for i,v in enumerate(importer_set)} \n",
    "hs6_mapping = {v:i+1 for i,v in enumerate(item_set)}\n",
    "importer_size = len(importer_mapping) + 1\n",
    "item_size = len(hs6_mapping) + 1\n",
    "# label-encoding\n",
    "train_importers = [importer_mapping[x] for x in train_raw_importers]\n",
    "train_items = [hs6_mapping[x] for x in train_raw_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test data, we use padding_idx=0 for unseen data\n",
    "# use dic.get(key,deafault) to handle unseen\n",
    "valid_importers = [importer_mapping.get(x,0) for x in valid_raw_importers]\n",
    "valid_items = [hs6_mapping.get(x,0) for x in valid_raw_items]\n",
    "test_importers = [importer_mapping.get(x,0) for x in test_raw_importers] \n",
    "test_items = [hs6_mapping.get(x,0) for x in test_raw_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. \"process_leaf_idx\" function [(link to practice)](#practice5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_leaf_idx(X_leaves): \n",
    "    '''\n",
    "    This function is to convert the output of XGBoost model to the input of DATE model.\n",
    "    For an individual import, the output of XGBoost model is a list of leaf index of multiple trees.\n",
    "    eg. [1, 1, 10, 9, 30, 30, 32, ... ]\n",
    "    How to distinguish \"node 1\" of the first tree from \"node 1\" of the second tree?\n",
    "    How to distinguish \"node 30\" of the fifth tree from \"node 30\" of the sixth tree?\n",
    "    This function is to assign unique index to every leaf node in all the trees. \n",
    "    This function returns;\n",
    "    - lists of unique leaf index;\n",
    "    - total number of unique leaf nodes; and\n",
    "    - a reference table (dictionary) composed of \"unique leaf index\", \"tree id\", \"(previous) leaf index\". \n",
    "    '''\n",
    "    leaves = X_leaves.copy()\n",
    "    new_leaf_index = dict() # dictionary to store leaf index\n",
    "    total_leaves = 0\n",
    "    for c in range(X_leaves.shape[1]): # iterate for each column (ie. 100 trees)\n",
    "        column = X_leaves[:,c]\n",
    "        unique_vals = list(sorted(set(column)))\n",
    "        new_idx = {v:(i+total_leaves) for i,v in enumerate(unique_vals)}\n",
    "        for i,v in enumerate(unique_vals):\n",
    "            leaf_id = i+total_leaves\n",
    "            new_leaf_index[leaf_id] = {c:v}\n",
    "        leaves[:,c] = [new_idx[v] for v in column]\n",
    "        total_leaves += len(unique_vals)\n",
    "        \n",
    "    assert leaves.ravel().max() == total_leaves - 1\n",
    "    return leaves,total_leaves,new_leaf_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. \"fgsm_attack\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, loss, images, labels, eps) :\n",
    "    # images.requires_grad = True\n",
    "    images = Variable(images, requires_grad=True)\n",
    "    outputs = model.module.pred_from_hidden(images)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    cost = loss(outputs, labels)\n",
    "    cost.backward()\n",
    "    attack_images = images + eps * images.grad.sign()\n",
    "    # attack_images = images + eps * F.normalize(images.grad.data, dim=0, p=2)\n",
    "    # attack_images.requires_grad = False\n",
    "    return attack_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. \"metrics\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_prob,xgb_testy,revenue_test,best_thresh=None):\n",
    "    if best_thresh ==None:\n",
    "        _,overall_f1,auc = torch_threshold(y_prob,xgb_testy,best_thresh)\n",
    "    else:\n",
    "        overall_f1,auc = torch_threshold(y_prob,xgb_testy,best_thresh)\n",
    "     # Seized revenue \n",
    "    # Precision and Recall\n",
    "    pr, re, f, rev = [], [], [], []\n",
    "    for i in [99,98,95,90]:\n",
    "        threshold = np.percentile(y_prob, i)\n",
    "        #print(f'Checking top {100-i}% suspicious transactions: {len(y_prob[y_prob > threshold])}')\n",
    "        precision = np.mean(xgb_testy[y_prob > threshold])\n",
    "        recall = sum(xgb_testy[y_prob > threshold])/sum(xgb_testy)\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "        revenue_recall = sum(revenue_test[y_prob > threshold]) /sum(revenue_test)\n",
    "\n",
    "        # save results\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "        f.append(f1)\n",
    "        rev.append(revenue_recall)\n",
    "        # print(f'Precision: {round(precision, 4)}, Recall: {round(recall, 4)}, Seized Revenue (Recall): {round(revenue_recall, 4)}')\n",
    "    return overall_f1,auc,pr, re, f, rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4. \"torch_threshold\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_threshold(y_pred_prob,y_test,best_thresh = None):\n",
    "    '''\n",
    "    This function is to find the best threshold to determine \"to inspect\" or \"not\".\n",
    "    We assume that we inspect only the imports where predicted probability of fraud is above the threshold.\n",
    "    '''\n",
    "    threshold_list = np.arange(0.1,0.6,0.1)\n",
    "    best_f1 = 0\n",
    "    if best_thresh == None:\n",
    "        for th in threshold_list:\n",
    "            y_pred_label = (y_pred_prob > th)*1 \n",
    "            f_score = f1_score(y_test,y_pred_label)\n",
    "            if f_score > best_f1:\n",
    "                best_f1 = f_score\n",
    "                best_thresh = th \n",
    "        return best_thresh, best_f1, roc_auc_score(y_test, y_pred_prob)\n",
    "    else:\n",
    "        y_pred_label = (y_pred_prob > best_thresh)*1 \n",
    "        best_f1 = f1_score(y_test,y_pred_label)\n",
    "        return best_f1, roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Identify leaf nodes of individual import from XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "\n",
    "# get leaf index from xgboost model \n",
    "X_train_leaves = xgb_clf.apply(xgb_trainx) #apply: Return the predicted leaf every tree for each sample.\n",
    "X_valid_leaves = xgb_clf.apply(xgb_validx)\n",
    "X_test_leaves = xgb_clf.apply(xgb_testx)\n",
    "train_rows = X_train_leaves.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "train_rows = train.shape[0]\n",
    "valid_rows = valid.shape[0] + train_rows\n",
    "X_leaves = np.concatenate((X_train_leaves, X_valid_leaves, X_test_leaves), axis=0) # make sure the dimensionality\n",
    "transformed_leaves, leaf_num, new_leaf_index = process_leaf_idx(X_leaves)\n",
    "train_leaves, valid_leaves, test_leaves = transformed_leaves[:train_rows],\\\n",
    "                                          transformed_leaves[train_rows:valid_rows],\\\n",
    "                                          transformed_leaves[valid_rows:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Convert data to tensor\n",
    "Tensor is a collection of numbers with specific shape (dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch type\n",
    "train_leaves = torch.tensor(train_leaves).long()\n",
    "train_user = torch.tensor(train_importers).long()\n",
    "train_item = torch.tensor(train_items).long()\n",
    "\n",
    "valid_leaves = torch.tensor(valid_leaves).long()\n",
    "valid_user = torch.tensor(valid_importers).long()\n",
    "valid_item = torch.tensor(valid_items).long()\n",
    "\n",
    "test_leaves = torch.tensor(test_leaves).long()\n",
    "test_user = torch.tensor(test_importers).long()\n",
    "test_item = torch.tensor(test_items).long()\n",
    "\n",
    "# cls data\n",
    "train_label_cls = torch.tensor(xgb_trainy).float()\n",
    "valid_label_cls = torch.tensor(xgb_validy).float()\n",
    "test_label_cls = torch.tensor(xgb_testy).float()\n",
    "\n",
    "# revenue data \n",
    "train_label_reg = torch.tensor(norm_revenue_train).float()\n",
    "valid_label_reg = torch.tensor(revenue_valid).float()\n",
    "test_label_reg = torch.tensor(revenue_test).float()\n",
    "\n",
    "# create dataloader \n",
    "\n",
    "train_dataset = Data.TensorDataset(train_leaves,train_user,train_item,train_label_cls,train_label_reg)\n",
    "valid_dataset = Data.TensorDataset(valid_leaves,valid_user,valid_item,valid_label_cls,valid_label_reg)\n",
    "test_dataset = Data.TensorDataset(test_leaves,test_user,test_item,test_label_cls,test_label_reg)\n",
    "\n",
    "\n",
    "\n",
    "data4embedding = {\"train_dataset\":train_dataset,\"valid_dataset\":valid_dataset,\"test_dataset\":test_dataset,\\\n",
    "                  \"leaf_num\":leaf_num,\"importer_num\":importer_size,\"item_size\":item_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Save and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1. Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"torch_data_test.pickle\", 'wb') as f:\n",
    "    pickle.dump(data4embedding, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"leaf_index.pickle\", \"wb\") as f:\n",
    "    pickle.dump(new_leaf_index, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load torch dataset \n",
    "with open(\"torch_data_test.pickle\",\"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get torch dataset \n",
    "train_dataset = data[\"train_dataset\"]\n",
    "valid_dataset = data[\"valid_dataset\"]\n",
    "test_dataset = data[\"test_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "batch_size = 256 * 8\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=True,               \n",
    ")\n",
    "valid_loader = Data.DataLoader(\n",
    "    dataset=valid_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=False,               \n",
    ")\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=test_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=False,               \n",
    ")\n",
    "\n",
    "# parameters for model \n",
    "leaf_num = data[\"leaf_num\"]\n",
    "importer_size = data[\"importer_num\"]\n",
    "item_size = data[\"item_size\"]\n",
    "\n",
    "# global variables\n",
    "xgb_validy = valid_loader.dataset.tensors[-2].detach().numpy()\n",
    "xgb_testy = test_loader.dataset.tensors[-2].detach().numpy()\n",
    "revenue_valid = valid_loader.dataset.tensors[-1].detach().numpy()\n",
    "revenue_test = test_loader.dataset.tensors[-1].detach().numpy()\n",
    "\n",
    "# model information\n",
    "curr_time = str(time.time())\n",
    "model_name = \"DATE\"\n",
    "model_path = \"./DATE_models/%s%s.pkl\" % (model_name,curr_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torch_multi_head_attention -> https://github.com/CyberZHG/torch-multi-head-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import numpy as np \n",
    "from torch_multi_head_attention import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice Mish()](#practice6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mish,self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x *( torch.tanh(F.softplus(x))) #softplus???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice FusionAttention()](#practice7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim):\n",
    "        super(FusionAttention, self).__init__()\n",
    "        self.attention_matrix = nn.Linear(dim, dim) # nn.Linear(size of input, size of output)\n",
    "        self.project_weight = nn.Linear(dim,1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        query_project = self.attention_matrix(inputs) # (b,t,d) -> (b,t,d2)\n",
    "        query_project = F.leaky_relu(query_project)\n",
    "        project_value = self.project_weight(query_project) # (b,t,h) -> (b,t,1)\n",
    "        attention_weight = torch.softmax(project_value, dim=1) # Normalize and calculate weights (b,t,1)\n",
    "        attention_vec = inputs * attention_weight\n",
    "        attention_vec = torch.sum(attention_vec,dim=1)\n",
    "        return attention_vec, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice Attention()](#practice8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim,hidden,aggregate=\"sum\"):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_matrix = nn.Linear(dim, hidden)\n",
    "        self.project_weight = nn.Linear(hidden*2,hidden)\n",
    "        self.h = nn.Parameter(torch.rand(hidden,1))\n",
    "        self.agg_type = aggregate\n",
    "        \n",
    "    def forward(self, query, key): # query: 256 X 16, # key: 256 X 100 X 16, # assume key==value\n",
    "        dim = query.size(-1) # 16 (n_embedding_dimension)\n",
    "        batch = key.size(0) # 256 (batch_size = n_observation in a batch)\n",
    "        time_step = key.size(1) # 100 (n_trees from xgboot model)\n",
    "        \n",
    "        # concate input query and key \n",
    "        query = query.view(batch,1,dim) # view = reshape: (256X16) -> (256X1X16)\n",
    "        query = query.expand(batch,time_step,-1) # expand to the same dimension: (256X1X16) -> (256X100X16)\n",
    "        cat_vector = torch.cat((query,key),dim=-1) # (256X100X32)\n",
    "        \n",
    "        # project to single value\n",
    "        project_vector = self.project_weight(cat_vector) \n",
    "        project_vector = torch.relu(project_vector)\n",
    "        attention_alpha = torch.matmul(project_vector,self.h)\n",
    "        attention_weight = torch.softmax(attention_alpha, dim=1) # Normalize and calculate weights (b,t,1)\n",
    "        attention_vec = key * attention_weight\n",
    "        \n",
    "        # aggregate leaves\n",
    "        if self.agg_type == \"max\":\n",
    "            attention_vec,_ = torch.max(attention_vec,dim=1)\n",
    "        elif self.agg_type ==\"mean\":\n",
    "            attention_vec = torch.mean(attention_vec,dim=1)\n",
    "        elif self.agg_type ==\"sum\":\n",
    "            attention_vec = torch.sum(attention_vec,dim=1)\n",
    "        return attention_vec, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice DATE()](#practice9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATE(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 max_leaf,importer_size,item_size,dim,\n",
    "                 head_num=4,fusion_type=\"concat\",act=\"relu\",device=0,use_self=True,agg_type=\"sum\"):\n",
    "        super(DATE, self).__init__()\n",
    "        self.d = dim\n",
    "        self.device = device\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.LeakyReLU()\n",
    "        elif act == \"mish\":\n",
    "            self.act = Mish() \n",
    "        self.fusion_type = fusion_type\n",
    "        self.use_self = use_self\n",
    "\n",
    "        # embedding layers \n",
    "        self.leaf_embedding = nn.Embedding(max_leaf,dim)\n",
    "        self.user_embedding = nn.Embedding(importer_size,dim,padding_idx=0)\n",
    "        self.user_embedding.weight.data[0] = torch.zeros(dim) # unseen data? initial value?\n",
    "        self.item_embedding = nn.Embedding(item_size,dim,padding_idx=0)\n",
    "        self.item_embedding.weight.data[0] = torch.zeros(dim)\n",
    "\n",
    "        # attention layer\n",
    "        self.attention_block = Attention(dim,dim,agg_type).to(device)\n",
    "        self.self_att = MultiHeadAttention(dim,head_num).to(device)\n",
    "        self.fusion_att = FusionAttention(dim)\n",
    "\n",
    "        # Hidden & output layer\n",
    "        self.layer_norm = nn.LayerNorm((100,dim))\n",
    "        self.fussionlayer = nn.Linear(dim*3,dim)\n",
    "        self.hidden = nn.Linear(dim,dim)\n",
    "        self.output_cls_layer = nn.Linear(dim,1)\n",
    "        self.output_reg_layer = nn.Linear(dim,1)\n",
    "    \n",
    "    def forward(self,feature,uid,item_id):\n",
    "        \n",
    "        # Embedding of leaf_id\n",
    "        leaf_vectors = self.leaf_embedding(feature)\n",
    "        \n",
    "        # 1st attention: Multi-Head Self-Attention\n",
    "        # Calculate the weight(importance) of each leaf(cross-feature) based on the correlation with other leafs\n",
    "        if self.use_self:\n",
    "            # Apply multy head attention\n",
    "            leaf_vectors = self.self_att(leaf_vectors,leaf_vectors,leaf_vectors)\n",
    "            # Normalization?\n",
    "            leaf_vectors = self.layer_norm(leaf_vectors)\n",
    "        \n",
    "        # Embedding of importer_id\n",
    "        importer_vector = self.user_embedding(uid)\n",
    "        # Embedding of item_id\n",
    "        item_vector = self.item_embedding(item_id)\n",
    "        # Multiply embeddings of importer_id and item_id\n",
    "        query_vector = importer_vector * item_vector\n",
    "        \n",
    "        # 2nd attention: Attention with leaf_id, importer_id and item_id (all embeddings)\n",
    "        set_vector, self.attention_w = self.attention_block(query_vector,leaf_vectors)\n",
    "        \n",
    "        # concat the user, item and tree vectors into a fusion attention\n",
    "        if self.fusion_type == \"concat\":\n",
    "            fusion = torch.cat((importer_vector, item_vector, set_vector), dim=-1) # attach as columns\n",
    "            fusion = self.act(self.fussionlayer(fusion))\n",
    "        elif self.fusion_type == \"attention\":\n",
    "            importer_vector=importer_vector.view(-1,1,self.d), \n",
    "            item_vector=item_vector.view(-1,1,self.d), \n",
    "            set_vector=set_vector.view(-1,1,self.d)\n",
    "            fusion = torch.cat((importer_vector, item_vector, set_vector), dim=1) # attach as columns\n",
    "            fusion,_ = self.fusion_att(fusion)\n",
    "        else:\n",
    "            raise \"Fusion type error\"\n",
    "        hidden = self.hidden(fusion)\n",
    "        hidden = self.act(hidden)\n",
    "\n",
    "        # multi-task output \n",
    "        classification_output = torch.sigmoid(self.output_cls_layer(hidden))\n",
    "        regression_output = torch.relu(self.output_reg_layer(hidden))\n",
    "        return classification_output, regression_output, hidden\n",
    "\n",
    "    def pred_from_hidden(self,hidden):\n",
    "        classification_output = torch.sigmoid(self.output_cls_layer(hidden))\n",
    "        return classification_output \n",
    "\n",
    "    def eval_on_batch(self,test_loader): # predict test data using batch \n",
    "        final_output = []\n",
    "        cls_loss = []\n",
    "        reg_loss = []\n",
    "        for batch in test_loader:\n",
    "            batch_feature, batch_user, batch_item, batch_cls, batch_reg = batch\n",
    "            batch_feature,batch_user,batch_item,batch_cls,batch_reg =  \\\n",
    "            batch_feature.to(self.device), batch_user.to(self.device),\\\n",
    "            batch_item.to(self.device), batch_cls.to(self.device), batch_reg.to(self.device)\n",
    "            batch_cls,batch_reg = batch_cls.view(-1,1), batch_reg.view(-1,1)\n",
    "            y_pred_prob, y_pred_rev,_ = self.forward(batch_feature,batch_user,batch_item)\n",
    "\n",
    "            # compute classification loss\n",
    "            cls_losses = nn.BCEWithLogitsLoss()(y_pred_prob,batch_cls) #nn.BCELoss()(y_pred_prob,batch_cls)\n",
    "            cls_loss.append(cls_losses.item())\n",
    "\n",
    "            # compute regression loss \n",
    "            reg_losses = nn.MSELoss()(y_pred_rev, batch_reg)\n",
    "            reg_loss.append(reg_losses.item())\n",
    "\n",
    "            # store predicted probability \n",
    "            y_pred = y_pred_prob.detach().cpu().numpy().tolist()\n",
    "            final_output.extend(y_pred)\n",
    "\n",
    "        print(\"CLS loss: %.4f, REG loss: %.4f\"% (np.mean(cls_loss), np.mean(reg_loss)) )\n",
    "        return np.array(final_output).ravel(), np.mean(cls_loss)+ np.mean(reg_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. Train (hyperparameter + loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.1. Set hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--save'], dest='save', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help='save model or not', metavar=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', \n",
    "                        type=str, \n",
    "                        default=\"DATE\", \n",
    "                        help=\"Name of model\",\n",
    "                        )\n",
    "parser.add_argument('--epoch', \n",
    "                        type=int, \n",
    "                        default=5, \n",
    "                        help=\"Number of epochs\",\n",
    "                        )\n",
    "parser.add_argument('--dim', \n",
    "                        type=int, \n",
    "                        default=500, \n",
    "                        help=\"Hidden layer dimension\",\n",
    "                        )\n",
    "parser.add_argument('--lr', \n",
    "                        type=float, \n",
    "                        default=0.005, \n",
    "                        help=\"learning rate\",\n",
    "                        )\n",
    "parser.add_argument('--l2',\n",
    "                        type=float,\n",
    "                        default=0.00,\n",
    "                        help=\"l2 reg\",\n",
    "                        )\n",
    "parser.add_argument('--alpha',\n",
    "                        type=float,\n",
    "                        default=10,\n",
    "                        help=\"Regression loss weight\",\n",
    "                        )\n",
    "parser.add_argument('--beta', type=float, default=0.00, help=\"Adversarial loss weight\")\n",
    "parser.add_argument('--head_num', type=int, default=4, help=\"Number of heads for self attention\")\n",
    "parser.add_argument('--use_self', type=int, default=1, help=\"Wheter to use self attention\")\n",
    "parser.add_argument('--fusion', type=str, choices=[\"concat\",\"attention\"], default=\"concat\", help=\"Fusion method for final embedding\")\n",
    "parser.add_argument('--agg', type=str, choices=[\"sum\",\"max\",\"mean\"], default=\"sum\", help=\"Aggreate type for leaf embedding\")\n",
    "parser.add_argument('--act', type=str, choices=[\"mish\",\"relu\"], default=\"mish\", help=\"Activation function\")\n",
    "parser.add_argument('--device', type=str, choices=[\"cuda:0\",\"cuda:1\",\"cpu\"], default=\"cuda:0\", help=\"device name for training\")\n",
    "parser.add_argument('--output', type=str, default=\"full.csv\", help=\"Name of output file\")\n",
    "parser.add_argument('--save', type=int, default=0, help=\"save model or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])\n",
    "# Why an empty list?\n",
    "# Refere to: https://stackoverflow.com/questions/50763033/argparse-in-jupyter-notebook-throws-a-typeerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare directory for saving results.\n",
    "import os\n",
    "if not os.path.exists('./DATE_models'):\n",
    "    os.makedirs('./DATE_models')\n",
    "if not os.path.exists('./DATE_results'):\n",
    "    os.makedirs('./DATE_results')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/cuda-error-device-side-assert-triggered-c6ae1c8fa4c3\n",
    "# https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2060'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(0 if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mish'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # get configs\n",
    "    epochs = args.epoch\n",
    "    dim = args.dim\n",
    "    lr = args.lr\n",
    "    weight_decay = args.l2\n",
    "    head_num = args.head_num\n",
    "    device = torch.device(0 if torch.cuda.is_available() else \"cpu\") #'cuda'#int(args.device.split(':')[1]) #'cpu' \n",
    "    act = args.act\n",
    "    fusion = args.fusion\n",
    "    beta = args.beta\n",
    "    alpha = args.alpha\n",
    "    use_self = args.use_self\n",
    "    agg = args.agg\n",
    "    model = DATE(leaf_num,importer_size,item_size,\\\n",
    "                                    dim,head_num,\\\n",
    "                                    fusion_type=fusion,act=act,device=device,\\\n",
    "                                    use_self=use_self,agg_type=agg,\n",
    "                                    ).to(device)\n",
    "    model = nn.DataParallel(model, device_ids=[0]) # device_ids=[0,1]\n",
    "\n",
    "    # initialize parameters\n",
    "    # Fills the input Tensor with values according to the method described in \n",
    "    # Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), \n",
    "    # using a uniform distribution.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # optimizer & loss \n",
    "    #from pytorch_forecasting.optim import Ranger\n",
    "    # https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer # WORKED 2021\n",
    "    \n",
    "    optimizer = Ranger(model.parameters(), weight_decay=weight_decay, lr=lr)\n",
    "    cls_loss_func = nn.BCEWithLogitsLoss() #nn.BCELoss()\n",
    "    reg_loss_func = nn.MSELoss()\n",
    "\n",
    "    # save best model\n",
    "    global_best_score = 0\n",
    "    model_state = None\n",
    "\n",
    "    # early stop settings \n",
    "    stop_rounds = 3\n",
    "    no_improvement = 0\n",
    "    current_score = None \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step, (batch_feature,batch_user,batch_item,batch_cls,batch_reg) in enumerate(train_loader):\n",
    "            model.train() # prep to train model\n",
    "            batch_feature,batch_user,batch_item,batch_cls,batch_reg =  \\\n",
    "            batch_feature.to(device), batch_user.to(device), batch_item.to(device),\\\n",
    "             batch_cls.to(device), batch_reg.to(device)\n",
    "            batch_cls,batch_reg = batch_cls.view(-1,1), batch_reg.view(-1,1)\n",
    "\n",
    "            # model output\n",
    "            classification_output, regression_output, hidden_vector = model(batch_feature,batch_user,batch_item)\n",
    "\n",
    "            # FGSM attack\n",
    "            adv_vector = fgsm_attack(model,cls_loss_func,hidden_vector,batch_cls,0.01)\n",
    "            adv_output = model.module.pred_from_hidden(adv_vector) \n",
    "\n",
    "            # calculate loss\n",
    "            adv_loss_func = nn.BCELoss(weight=batch_cls) \n",
    "            adv_loss = beta * adv_loss_func(adv_output,batch_cls) \n",
    "            cls_loss = cls_loss_func(classification_output,batch_cls)\n",
    "            revenue_loss = alpha * reg_loss_func(regression_output, batch_reg)\n",
    "            loss = cls_loss + revenue_loss + adv_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step+1) % 1000 ==0:  \n",
    "                print(\"CLS loss:%.4f, REG loss:%.4f, ADV loss:%.4f, Loss:%.4f\"\\\n",
    "                %(cls_loss.item(),revenue_loss.item(),adv_loss.item(),loss.item()))\n",
    "                \n",
    "        # evaluate \n",
    "        model.eval()\n",
    "        print(\"Validate at epoch %s\"%(epoch+1))\n",
    "        y_prob, val_loss = model.module.eval_on_batch(valid_loader)\n",
    "        y_pred_tensor = torch.tensor(y_prob).float().to(device)\n",
    "        best_threshold, val_score, roc = torch_threshold(y_prob,xgb_validy)\n",
    "        overall_f1, auc, precisions, recalls, f1s, revenues = metrics(y_prob,xgb_validy,revenue_valid)\n",
    "        select_best = np.mean(f1s)\n",
    "        print(\"Over-all F1:%.4f, AUC:%.4f, F1-top:%.4f\" % (overall_f1, auc, select_best) )\n",
    "\n",
    "        print(\"Evaluate at epoch %s\"%(epoch+1))\n",
    "        y_prob, val_loss = model.module.eval_on_batch(test_loader)\n",
    "        y_pred_tensor = torch.tensor(y_prob).float().to(device)\n",
    "        overall_f1, auc, precisions, recalls, f1s, revenues = metrics(y_prob,xgb_testy,revenue_test,best_thresh=best_threshold)\n",
    "        print(\"Over-all F1:%.4f, AUC:%.4f, F1-top:%.4f\" %(overall_f1, auc, np.mean(f1s)) )\n",
    "\n",
    "        # save best model \n",
    "        if select_best > global_best_score:\n",
    "            global_best_score = select_best\n",
    "            torch.save(model,model_path)\n",
    "        \n",
    "         # early stopping \n",
    "        if current_score == None:\n",
    "            current_score = select_best\n",
    "            continue\n",
    "        if select_best < current_score:\n",
    "            current_score = select_best\n",
    "            no_improvement += 1\n",
    "        if no_improvement >= stop_rounds:\n",
    "            print(\"Early stopping...\")\n",
    "            break \n",
    "        if select_best > current_score:\n",
    "            no_improvement = 0\n",
    "            current_score = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-83-8500103bf6c5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0muse_self\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0magg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     model = DATE(leaf_num,importer_size,item_size,\\\n\u001b[0m\u001b[0;32m     16\u001b[0m                                     \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhead_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                                     \u001b[0mfusion_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfusion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-76aee521a88d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, max_leaf, importer_size, item_size, dim, head_num, fusion_type, act, device, use_self, agg_type)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# attention layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_block\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magg_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_att\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhead_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfusion_att\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFusionAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\javia\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32mD:\\Users\\javia\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\javia\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\javia\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Evaluation <a id='part4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(save_model):\n",
    "    print()\n",
    "    print(\"--------Evaluating DATE model---------\")\n",
    "    # create best model\n",
    "    best_model = torch.load(model_path)\n",
    "    best_model.eval()\n",
    "\n",
    "    # get threshold\n",
    "    y_prob, val_loss = best_model.module.eval_on_batch(valid_loader)\n",
    "    best_threshold, val_score, roc = torch_threshold(y_prob,xgb_validy)\n",
    "\n",
    "    # predict test \n",
    "    y_prob, val_loss = best_model.module.eval_on_batch(test_loader)\n",
    "    overall_f1, auc, precisions, recalls, f1s, revenues = metrics(y_prob,xgb_testy,revenue_test,best_threshold)\n",
    "    best_score = f1s[0]\n",
    "    #os.system(\"rm %s\"%model_path)\n",
    "    if save_model:\n",
    "        scroed_name = \"./DATE_models/%s_%.4f.pkl\" % (model_name,overall_f1)\n",
    "        torch.save(best_model,scroed_name)\n",
    "    \n",
    "    return overall_f1, auc, precisions, recalls, f1s, revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_f1, auc, precisions, recalls, f1s, revenues = evaluate(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overall_f1, auc, precisions, recalls, f1s, revenues = evaluate(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "output_file =  \"./DATE_results/full.csv\"\n",
    "print(\"Saving result...\",output_file)\n",
    "with open(output_file, 'a') as ff:\n",
    "    # print(args,file=ff)\n",
    "    print()\n",
    "    print(\"\"\"Metrics:\\nf1:%.4f auc:%.4f\\nPr@1:%.4f Pr@2:%.4f Pr@5:%.4f Pr@10:%.4f\\nRe@1:%.4f Re@2:%.4f Re@5:%.4f Re@10:%.4f\\nRev@1:%.4f Rev@2:%.4f Rev@5:%.4f Rev@10:%.4f\"\"\" \\\n",
    "          % (overall_f1, auc,\\\n",
    "             precisions[0],precisions[1],precisions[2],precisions[3],\\\n",
    "             recalls[0],recalls[1],recalls[2],recalls[3],\\\n",
    "             revenues[0],revenues[1],revenues[2],revenues[3]\n",
    "            ),\n",
    "        ) \n",
    "    output_metric = [16,overall_f1,auc] + precisions + recalls + revenues\n",
    "    output_metric = list(map(str,output_metric))\n",
    "    print(\" \".join(output_metric),file=ff)\n",
    "        \n",
    "    # print(\"Model:%s epoch:%d dim:%d lr:%f l2:%f beta:%f heads:%d fusion:%s activation:%s\"\n",
    "    #       % (model_name, epochs, dim, lr, weight_decay, beta, head_num,fusion,act),file=ff) \n",
    "    # print(\"\"\"Metrics:\\nf1:%.4f auc:%.4f\\nPr@1:%.4f Pr@2:%.4f Pr@5:%.4f Pr@10:%.4f\\nRe@1:%.4f Re@2:%.4f Re@5:%.4f Re@10:%.4f\\nRev@1:%.4f Rev@2:%.4f Rev@5:%.4f Rev@10:%.4f\"\"\"  \\\n",
    "    #       % (overall_f1, auc,\\\n",
    "    #          precisions[0],precisions[1],precisions[2],precisions[3],\\\n",
    "    #          recalls[0],recalls[1],recalls[2],recalls[3],\\\n",
    "    #          revenues[0],revenues[1],revenues[2],revenues[3]\n",
    "    #          ),\n",
    "    #          file=ff)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5. Comparison with XGBoost and Logistic Regression <a id='part5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1. Performance of XGBoost model\n",
    "Performance indicators;\n",
    "- precision = (number of seizures)/(number of inspections) --> targeting accuracy\n",
    "- recall = (number of seizures)/(number of actual frauds)\n",
    "- revenue_recall = (revenue from seizures)/(revenue from actual frauds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Precision and Recall\n",
    "y_prob = test_pred\n",
    "for i in [99,98,95,90]:\n",
    "    # Find the ith value in ascending order.\n",
    "    threshold = np.percentile(y_prob, i)\n",
    "    print(f'Checking top {100-i}% suspicious transactions: {len(y_prob[y_prob > threshold])}')\n",
    "    precision = np.mean(xgb_testy[y_prob > threshold])\n",
    "    recall = sum(xgb_testy[y_prob > threshold])/sum(xgb_testy)\n",
    "    revenue_recall = sum(revenue_test[y_prob > threshold]) /sum(revenue_test)\n",
    "    print(f'Precision: {round(precision, 4)}, Recall: {round(recall, 4)}, Seized Revenue (Recall): {round(revenue_recall, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the structured trees in txt file\n",
    "xgb_clf.get_booster().dump_model('xgb_model.txt', with_stats=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2. XGBoost + Logistic Regression model <a id='part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.1. Deploy Xgboost+LR model \n",
    "Summary\n",
    "    1. Apply the pre-trained XGBoost model to train-, valid- and test-data. \n",
    "    2. For an individual import, the XGBoost model returns a list of leaf index of the multiple trees. \n",
    "    3. One-hot-encode leaf node.\n",
    "    4. The encoded leaf index is fed as an input into the logistic regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Apply the pre-trained XGBoost model to train-, valid- and test-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost+LR model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "\n",
    "# get leaf index from xgboost model \n",
    "X_train_leaves = xgb_clf.apply(xgb_trainx) #apply: Return the predicted leaf every tree for each sample.\n",
    "X_valid_leaves = xgb_clf.apply(xgb_validx)\n",
    "X_test_leaves = xgb_clf.apply(xgb_testx)\n",
    "train_rows = X_train_leaves.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For an individual import, the XGBoost model returns a list of leaf index of the multiple trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the 100 leaf nodes of the first import in the train-data\n",
    "X_train_leaves[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. One-hot-encode the leaf (Create unique set of 0s and 1s for individual leaf index).\n",
    "For instance, \n",
    "    - Leaf index 1: [100000...00]\n",
    "    - Leaf index 2: [010000...00]\n",
    "    - Leaf index 3: [001000...00]\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding for leaf index\n",
    "xgbenc = OneHotEncoder(categories=\"auto\")\n",
    "lr_trainx = xgbenc.fit_transform(X_train_leaves)\n",
    "lr_validx = xgbenc.transform(X_valid_leaves)\n",
    "lr_testx = xgbenc.transform(X_test_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Input the encoded leaf index into the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "print(\"Training Logistic regression model...\")\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(lr_trainx, xgb_trainy)\n",
    "test_pred = lr.predict_proba(lr_testx)[:,1]\n",
    "print(\"------Evaluating xgboost+LR model------\")\n",
    "xgb_auc = roc_auc_score(xgb_testy, test_pred)\n",
    "xgb_threshold,_ = find_best_threshold(lr, lr_validx, xgb_validy) # threshold was select from validation set\n",
    "xgb_f1 = find_best_threshold(lr, lr_testx, xgb_testy,best_thresh=xgb_threshold) # then applied on test set\n",
    "print(\"AUC = %.4f, F1-score = %.4f\" % (xgb_auc, xgb_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.2. Performance of XGBoost + LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Precision and Recall\n",
    "y_prob = test_pred\n",
    "for i in [99,98,95,90]:\n",
    "    threshold = np.percentile(y_prob, i)\n",
    "    print(f'Checking top {100-i}% suspicious transactions: {len(y_prob[y_prob > threshold])}')\n",
    "    precision = np.mean(xgb_testy[y_prob > threshold])\n",
    "    recall = sum(xgb_testy[y_prob > threshold])/sum(xgb_testy)\n",
    "    revenue_recall = sum(revenue_test[y_prob > threshold]) /sum(revenue_test)\n",
    "    print(f'Precision: {round(precision, 4)}, Recall: {round(recall, 4)}, Seized Revenue (Recall): {round(revenue_recall, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6. Practice of functions <a id='part6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge_attributes <a id='practice1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a small data for the demonstration.\n",
    "df_sample = df.sample(3)\n",
    "merge_attributes(df_sample, 'inciso_arancelario','pais_origen_destino')\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can identify that new column \"TARIFF.CODE&ORIGIN.CODE\" has been created.  \n",
    "You can learn each step of the function by replicating the codes line by line as follows;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(3)\n",
    "print('Check what happens from \"iterables = [df[arg].astype(str) for arg in args]\"\\n=====')\n",
    "iterables = [df_sample[arg].astype(str) for arg in ['TARIFF.CODE','ORIGIN.CODE']] \n",
    "print(iterables)\n",
    "print('')\n",
    "print('Check what happens from \"zip(*iterables)\"\\n=====')\n",
    "print(list(zip(*iterables)))\n",
    "print('')\n",
    "print('What happens from \"fs = [''.join([v for v in var]) for var in zip(*iterables)]\" - part1\\n=====')\n",
    "print(list(var for var in zip(*iterables)))\n",
    "print('')\n",
    "print('What happens from \"fs = [''.join([v for v in var]) for var in zip(*iterables)]\" - part2\\n=====')\n",
    "fs = [''.join([v for v in var]) for var in zip(*iterables)]\n",
    "print(fs)\n",
    "print('')\n",
    "print('What happens from \"columnName = \\'&\\'.join([*args])\\'&\\'.join([*args])\"\\n=====')\n",
    "columnName = '&'.join(['TARIFF.CODE','ORIGIN.CODE'])\n",
    "print(columnName)\n",
    "print('')\n",
    "df_sample.loc[:, columnName] = fs\n",
    "print('df_sample[columnName]\\n=====')\n",
    "print(df_sample[columnName])\n",
    "\n",
    "# Now, we finish the line-by-line practice of the \"merge_attributes\" fuction, and delete temporarily created data. \n",
    "\n",
    "del df_sample, iterables, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess <a id='practice2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(3)\n",
    "df_sample = preprocess(df_sample)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_risk_profile <a id='practice3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(100)\n",
    "\n",
    "# We will identify top 10% of high risk importers in terms of its non-compliance records. \n",
    "find_risk_profile(df=df_sample, feature='IMPORTER.TIN', topk_ratio=0.1, adj=10, option='topk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tag_risky_profiles <a id='practice4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a sample data\n",
    "df_sample = df.sample(100)\n",
    "# Identify top 10% high-risk importer\n",
    "risk_profiles = find_risk_profile(df_sample, 'IMPORTER.TIN', 0.1, 10, option='topk')\n",
    "risk_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag \"RiskH\" to the top 10% high-risk importers\n",
    "aaa = tag_risky_profiles(df=df_sample, profile='IMPORTER.TIN', profiles=risk_profiles, option='topk')\n",
    "aaa.sort_values('RiskH.IMPORTER.TIN', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample, aaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_leaf_idx <a id='practice5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = X_train_leaves[:3] # 100 trees in our xgboost model!\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves, total_leaves, new_leaf_index = process_leaf_idx(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new idex of leaf\n",
    "leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reference: {new_leaf_index: {tree_index: leaf_index}}\n",
    "new_leaf_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample, leaves, total_leaves, new_leaf_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mish <a id='practice6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,3))\n",
    "Mish_test = Mish()\n",
    "print(\"x: \", x,\"Mish(x): \", Mish_test(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FusionAttention <a id='practice7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('0. Set an arbitrary imput data\\n=====')\n",
    "inputs_test = torch.randn(256,3,16)\n",
    "print(inputs_test.shape)\n",
    "dim_test=inputs_test.shape[-1]\n",
    "print('')\n",
    "print(\"1. query_project \\n==============\")\n",
    "query_project = nn.Linear(dim_test,dim_test)(inputs_test)\n",
    "print(query_project.shape)\n",
    "print('')\n",
    "print(\"2. query_project \\n==============\")\n",
    "query_project = F.leaky_relu(query_project)\n",
    "print(query_project.shape)\n",
    "print('')\n",
    "print(\"3. project_value \\n==============\")\n",
    "project_value = nn.Linear(dim_test,1)(query_project)\n",
    "print(project_value.shape)\n",
    "print('')\n",
    "print(\"4. attention_weight \\n==============\")\n",
    "attention_weight = torch.softmax(project_value, dim=1)\n",
    "print(attention_weight.shape)\n",
    "print('')\n",
    "print(\"5. attention_vec \\n==============\")\n",
    "attention_vec = inputs_test*attention_weight\n",
    "print(attention_vec.shape)\n",
    "print('')\n",
    "print(\"6. attention_vec \\n==============\")\n",
    "attention_vec = torch.sum(attention_vec, dim=1)\n",
    "print(attention_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention <a id='practice8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1. Set arbitrary \"query\" and \"key\"\\n=====')\n",
    "# \"query\" is the embedding layer of importer_id + HScode_id in the final model.\n",
    "# \"key\" is the embedding layer of leaf_id of 100 decision trees in the final model.\n",
    "query = torch.randn(256, 16) # (batch_size, n_embedding_dim)\n",
    "print('query:',query.shape)\n",
    "key = torch.randn(256, 100, 16) # (batch_size, n_trees, n_embedding_dim)\n",
    "print('key:',key.shape)\n",
    "print('')\n",
    "print('2. Transform \"quary\" to be merged with \"key\"\\n=====')\n",
    "dim = query.size(-1) # 16 (n_embedding_dimension)\n",
    "batch = key.size(0) # 256 (batch_size = n_observation in a batch)\n",
    "time_step = key.size(1) # 100 (n_trees from xgboot model)\n",
    "query = query.view(batch,1,dim) # view = reshape: (256X16) -> (256X1X16)\n",
    "query = query.expand(batch,time_step,-1) # expand to the same dimension: (256X1X16) -> (256X100X16)\n",
    "print('query:', query.shape)\n",
    "cat_vector = torch.cat((query,key),dim=-1) # (256X100X32)\n",
    "print('cat_vector:',cat_vector.shape)\n",
    "print('')\n",
    "print('3. Neural Network\\n=====')\n",
    "project_vector = nn.Linear(32,16)(cat_vector)\n",
    "print('3-1. project_vector:',project_vector.shape)\n",
    "project_vector = torch.relu(project_vector)\n",
    "print('3-2. project_vector:',project_vector.shape)\n",
    "h = nn.Parameter(torch.rand(16,1))\n",
    "print('3-3. h:',h.shape)\n",
    "attention_alpha = torch.matmul(project_vector,h)\n",
    "attention_weight = torch.softmax(attention_alpha, dim=1) # Normalize and calculate weights (b,t,1)\n",
    "print('3-4. attention_weight:',attention_weight.shape)\n",
    "attention_vec = key * attention_weight # assumed 'key'=='value'\n",
    "print('3-5. attention_vec:',attention_vec.shape)\n",
    "print('')\n",
    "print('4. Result\\n=====')\n",
    "max_attention_vec,_ = torch.max(attention_vec,dim=1)\n",
    "print('4-1. max_attention_vec:',max_attention_vec.shape)\n",
    "mean_attention_vec = torch.mean(attention_vec,dim=1)\n",
    "print('4-2. mean_attention_vec:',mean_attention_vec.shape)\n",
    "sum_attention_vec = torch.sum(attention_vec,dim=1)\n",
    "print('4-3. sum_attention_vec:',sum_attention_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATE model <a id='practice9'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "head_num = 4\n",
    "\n",
    "print('1. Hyperparameters\\n=====')\n",
    "print('dim:',dim)\n",
    "print('leaf_num:',leaf_num)\n",
    "print('device:',device)\n",
    "print('item_size:',item_size)\n",
    "print('importer_size:',importer_size)\n",
    "print('')\n",
    "print('2. Toy data (batch)\\n=====\\none batch from \\'train_loader\\'')\n",
    "feature,uid,item_id,cls,reg = next(iter(train_loader))\n",
    "print('feature:',feature.shape)\n",
    "print('uid:',uid.shape)\n",
    "print('item_id:',item_id.shape)\n",
    "# other option\n",
    "#feature = train_loader.dataset.tensors[0][:256]\n",
    "#uid = train_loader.dataset.tensors[1][:256]\n",
    "#item_id = train_loader.dataset.tensors[2][:256]\n",
    "print('')\n",
    "print('3. Embedding of feature (cross-features, leaf_ids)\\n=====')\n",
    "leaf_vectors = nn.Embedding(leaf_num,dim)(feature)\n",
    "print('leaf_vectors:',leaf_vectors.shape)\n",
    "print('')\n",
    "print('4. Multi-Head Self-Attention (1st attention) \\n=====')\n",
    "# Calculate the weight(importance) of each leaf(cross-feature) based on the correlation with other leafs\n",
    "# Apply multy head attention\n",
    "multiheadattention = MultiHeadAttention(dim,head_num).to(device)\n",
    "leaf_vectors = multiheadattention(leaf_vectors,leaf_vectors,leaf_vectors)\n",
    "print(leaf_vectors.shape)\n",
    "leaf_vectors = nn.LayerNorm((100,dim))(leaf_vectors)\n",
    "print(leaf_vectors.shape)\n",
    "print('')\n",
    "print('5. Embedding of importer_id\\n=====')\n",
    "importer_vector = nn.Embedding(importer_size,dim,padding_idx=0)(uid)\n",
    "print(importer_vector.shape)\n",
    "print('')\n",
    "print('6. Embedding of item_id(HScode)\\n=====')\n",
    "item_vector = nn.Embedding(item_size,dim,padding_idx=0)(item_id)\n",
    "print(item_vector.shape)\n",
    "print(' ')\n",
    "print('7. Multiply embeddings of importer_id and item_id\\n=====')\n",
    "query_vector = importer_vector * item_vector\n",
    "print(query_vector.shape)\n",
    "print(' ')\n",
    "print('8. Attention with leaf_id, importer_id and item_id (2nd attention)\\n=====')\n",
    "attention = Attention(dim,dim,\"sum\").to(device)\n",
    "set_vector, _ = attention(query_vector,leaf_vectors)\n",
    "print(set_vector.shape)\n",
    "print('')\n",
    "print('9. Concat the user_id, item_id and set(attention)_vector to fusion\\n=====')\n",
    "importer_vector=importer_vector.view(-1,1,dim)\n",
    "print('importer_vector:',importer_vector.shape)\n",
    "item_vector=item_vector.view(-1,1,dim) \n",
    "print('item_vector:',item_vector.shape)\n",
    "set_vector=set_vector.view(-1,1,dim)\n",
    "print('set_vector:',set_vector.shape)\n",
    "fusion = torch.cat((importer_vector, item_vector, set_vector), dim=1) # attach as columns\n",
    "print('fusion:', fusion.shape)\n",
    "print('')\n",
    "print('10. Fusion Attention\\n=====')\n",
    "fusion_att = FusionAttention(dim)\n",
    "fusion,_ = fusion_att(fusion) # fusion attention\n",
    "print('fusion:',fusion.shape)\n",
    "print('')\n",
    "print('11. Task-specific layers\\n=====')\n",
    "hidden = nn.Linear(dim,dim)(fusion)\n",
    "hidden = nn.LeakyReLU()(hidden)\n",
    "print('hidden:',hidden.shape)\n",
    "print('')\n",
    "print('12. output\\n=====')\n",
    "classification_output = torch.sigmoid(nn.Linear(dim,1)(hidden))\n",
    "print('classification_output:',classification_output.shape)\n",
    "regression_output = torch.relu(nn.Linear(dim,1)(hidden))\n",
    "print('regression_output:',regression_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
